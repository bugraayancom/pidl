"""
Multi-LLM KarÅŸÄ±laÅŸtÄ±rma Motoru
OpenAI, Google Gemini, Anthropic Claude, X.AI Grok desteÄŸi

AraÅŸtÄ±rma Sorusu: FarklÄ± LLM'ler aynÄ± persona karakteristiÄŸini 
ne kadar iyi yansÄ±tÄ±yor ve performans farklarÄ± nedir?
"""

import os
from typing import Dict, List, Optional
from dotenv import load_dotenv

# LLM imports
import openai

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False

from personas import Persona

load_dotenv()


class MultiLLMEngine:
    """
    Ã‡oklu LLM desteÄŸi ile kod Ã¼retimi
    
    Desteklenen modeller:
    - OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
    - Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku
    - Google: gemini-pro, gemini-1.5-pro
    - X.AI: grok-beta
    """
    
    # Model fiyatlandÄ±rmasÄ± (1M token baÅŸÄ±na $)
    PRICING = {
        "gpt-4o": {"input": 5.00, "output": 15.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-4-turbo": {"input": 10.00, "output": 30.00},
        "claude-3-opus": {"input": 15.00, "output": 75.00},
        "claude-3-sonnet": {"input": 3.00, "output": 15.00},
        "claude-3-haiku": {"input": 0.25, "output": 1.25},
        "gemini-pro": {"input": 0.50, "output": 1.50},
        "gemini-1.5-pro": {"input": 3.50, "output": 10.50},
        "grok-beta": {"input": 5.00, "output": 15.00}
    }
    
    def __init__(self):
        """Multi-LLM engine baÅŸlat"""
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        self.google_key = os.getenv("GOOGLE_API_KEY")
        self.grok_key = os.getenv("GROK_API_KEY")
        
        # Clients
        self.openai_client = None
        self.anthropic_client = None
        self.google_client = None
        
        self._initialize_clients()
    
    def _initialize_clients(self):
        """LLM client'larÄ±nÄ± baÅŸlat"""
        # OpenAI
        if self.openai_key:
            try:
                self.openai_client = openai.OpenAI(api_key=self.openai_key)
            except Exception as e:
                print(f"OpenAI client baÅŸlatÄ±lamadÄ±: {e}")
        
        # Anthropic - Sadece geÃ§erli API key varsa baÅŸlat
        if self.anthropic_key and self.anthropic_key != "your_anthropic_api_key_here" and ANTHROPIC_AVAILABLE:
            try:
                self.anthropic_client = anthropic.Anthropic(api_key=self.anthropic_key)
            except Exception as e:
                print(f"Anthropic client baÅŸlatÄ±lamadÄ±: {e}")
                self.anthropic_client = None
        
        # Google - Sadece geÃ§erli API key varsa baÅŸlat
        if self.google_key and self.google_key != "your_google_key_here" and GOOGLE_AVAILABLE:
            try:
                genai.configure(api_key=self.google_key)
                self.google_client = genai
            except Exception as e:
                print(f"Google client baÅŸlatÄ±lamadÄ±: {e}")
                self.google_client = None
    
    def get_available_models(self) -> List[Dict]:
        """KullanÄ±labilir modelleri listele"""
        models = []
        
        if self.openai_client:
            models.extend([
                {"provider": "OpenAI", "model": "gpt-4o", "status": "âœ…"},
                {"provider": "OpenAI", "model": "gpt-4o-mini", "status": "âœ…"},
                {"provider": "OpenAI", "model": "gpt-4-turbo", "status": "âœ…"}
            ])
        
        if self.anthropic_client:
            models.extend([
                {"provider": "Anthropic", "model": "claude-3-opus-20240229", "status": "âœ…"},
                {"provider": "Anthropic", "model": "claude-3-sonnet-20240229", "status": "âœ…"},
                {"provider": "Anthropic", "model": "claude-3-haiku-20240307", "status": "âœ…"}
            ])
        
        if self.google_client:
            models.extend([
                {"provider": "Google", "model": "gemini-pro", "status": "âœ…"},
                {"provider": "Google", "model": "gemini-1.5-pro", "status": "âœ…"}
            ])
        
        # Grok (OpenAI compatible API)
        if self.grok_key:
            models.append(
                {"provider": "X.AI", "model": "grok-beta", "status": "âœ…"}
            )
        
        return models
    
    def generate_with_openai(self, model: str, system_prompt: str, 
                            user_prompt: str) -> Dict:
        """OpenAI ile kod Ã¼ret"""
        try:
            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            code = response.choices[0].message.content.strip()
            
            # Kod bloklarÄ±nÄ± temizle
            if code.startswith("```python"):
                code = code[9:]
            elif code.startswith("```"):
                code = code[3:]
            if code.endswith("```"):
                code = code[:-3]
            code = code.strip()
            
            return {
                "success": True,
                "code": code,
                "tokens": response.usage.total_tokens,
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens,
                "model": model,
                "provider": "OpenAI"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": model,
                "provider": "OpenAI"
            }
    
    def generate_with_anthropic(self, model: str, system_prompt: str,
                               user_prompt: str) -> Dict:
        """Anthropic Claude ile kod Ã¼ret"""
        try:
            if not self.anthropic_client:
                return {"success": False, "error": "Anthropic client yok"}
            
            response = self.anthropic_client.messages.create(
                model=model,
                max_tokens=2000,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            code = response.content[0].text.strip()
            
            # Kod bloklarÄ±nÄ± temizle
            if code.startswith("```python"):
                code = code[9:]
            elif code.startswith("```"):
                code = code[3:]
            if code.endswith("```"):
                code = code[:-3]
            code = code.strip()
            
            return {
                "success": True,
                "code": code,
                "tokens": response.usage.input_tokens + response.usage.output_tokens,
                "input_tokens": response.usage.input_tokens,
                "output_tokens": response.usage.output_tokens,
                "model": model,
                "provider": "Anthropic"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": model,
                "provider": "Anthropic"
            }
    
    def generate_with_google(self, model: str, system_prompt: str,
                            user_prompt: str) -> Dict:
        """Google Gemini ile kod Ã¼ret"""
        try:
            if not self.google_client:
                return {"success": False, "error": "Google client yok"}
            
            model_obj = self.google_client.GenerativeModel(model)
            
            # Gemini system instruction ile Ã§alÄ±ÅŸÄ±r
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            response = model_obj.generate_content(full_prompt)
            
            code = response.text.strip()
            
            # Kod bloklarÄ±nÄ± temizle
            if code.startswith("```python"):
                code = code[9:]
            elif code.startswith("```"):
                code = code[3:]
            if code.endswith("```"):
                code = code[:-3]
            code = code.strip()
            
            # Gemini token sayÄ±sÄ±nÄ± tam vermez, tahmin edelim
            estimated_tokens = len(full_prompt.split()) + len(code.split())
            
            return {
                "success": True,
                "code": code,
                "tokens": estimated_tokens,
                "input_tokens": len(full_prompt.split()),
                "output_tokens": len(code.split()),
                "model": model,
                "provider": "Google"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": model,
                "provider": "Google"
            }
    
    def generate_with_grok(self, model: str, system_prompt: str,
                          user_prompt: str) -> Dict:
        """X.AI Grok ile kod Ã¼ret (OpenAI compatible)"""
        try:
            if not self.grok_key:
                return {"success": False, "error": "Grok API key yok"}
            
            # Grok OpenAI compatible API kullanÄ±r
            client = openai.OpenAI(
                api_key=self.grok_key,
                base_url="https://api.x.ai/v1"
            )
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            code = response.choices[0].message.content.strip()
            
            # Kod bloklarÄ±nÄ± temizle
            if code.startswith("```python"):
                code = code[9:]
            elif code.startswith("```"):
                code = code[3:]
            if code.endswith("```"):
                code = code[:-3]
            code = code.strip()
            
            return {
                "success": True,
                "code": code,
                "tokens": response.usage.total_tokens,
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens,
                "model": model,
                "provider": "X.AI"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": model,
                "provider": "X.AI"
            }
    
    def generate_multi_llm(self, persona: Persona, persona_prompt: str,
                          models: List[str]) -> List[Dict]:
        """
        AynÄ± persona ve prompt ile birden fazla LLM'den kod Ã¼ret
        
        Args:
            persona: Persona objesi
            persona_prompt: Persona'nÄ±n yazdÄ±ÄŸÄ± Ã¶zel prompt
            models: KullanÄ±lacak model listesi
            
        Returns:
            Her model iÃ§in sonuÃ§ listesi
        """
        results = []
        
        # Tam user prompt
        user_prompt = f"""
GÃ¶rev iÃ§in senin perspektifinden hazÄ±rladÄ±ÄŸÄ±n prompt:
"{persona_prompt}"

Bu prompt'a gÃ¶re Python kodu yaz. Kodun:
- Ã‡alÄ±ÅŸabilir ve test edilebilir olmalÄ±
- Kendi uzmanlÄ±k alanÄ±na gÃ¶re yaklaÅŸmalÄ±sÄ±n
- Gerekli import'larÄ± ekle

Sadece Python kodunu yaz, baÅŸka aÃ§Ä±klama ekleme.
"""
        
        for model in models:
            # Provider'Ä± belirle
            if model.startswith("gpt"):
                result = self.generate_with_openai(model, persona.system_prompt, user_prompt)
            elif model.startswith("claude"):
                result = self.generate_with_anthropic(model, persona.system_prompt, user_prompt)
            elif model.startswith("gemini"):
                result = self.generate_with_google(model, persona.system_prompt, user_prompt)
            elif model.startswith("grok"):
                result = self.generate_with_grok(model, persona.system_prompt, user_prompt)
            else:
                result = {"success": False, "error": "Bilinmeyen model"}
            
            # Persona bilgilerini ekle
            result.update({
                "persona_id": persona.id,
                "persona_name": persona.name,
                "persona_role": persona.role,
                "category": persona.category,
                "avatar": persona.avatar,
                "persona_prompt": persona_prompt
            })
            
            results.append(result)
        
        return results
    
    def calculate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:
        """
        Maliyet hesapla
        
        Args:
            input_tokens: Input token sayÄ±sÄ±
            output_tokens: Output token sayÄ±sÄ±
            model: Model adÄ±
            
        Returns:
            Toplam maliyet ($)
        """
        # Model ismini normalize et (versiyonlarÄ± kaldÄ±r)
        model_key = model
        if "claude-3" in model:
            if "opus" in model:
                model_key = "claude-3-opus"
            elif "sonnet" in model:
                model_key = "claude-3-sonnet"
            elif "haiku" in model:
                model_key = "claude-3-haiku"
        elif "gemini" in model:
            if "1.5" in model:
                model_key = "gemini-1.5-pro"
            else:
                model_key = "gemini-pro"
        
        pricing = self.PRICING.get(model_key, {"input": 1.0, "output": 3.0})
        
        input_cost = (input_tokens / 1_000_000) * pricing["input"]
        output_cost = (output_tokens / 1_000_000) * pricing["output"]
        
        return input_cost + output_cost


# Test iÃ§in
if __name__ == "__main__":
    engine = MultiLLMEngine()
    
    available = engine.get_available_models()
    
    print("ðŸ¤– KullanÄ±labilir LLM Modelleri:\n")
    for model in available:
        print(f"{model['status']} {model['provider']:12s} - {model['model']}")
    
    print(f"\nâœ… Toplam {len(available)} model kullanÄ±labilir")

