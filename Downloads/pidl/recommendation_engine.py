"""
Matematiksel Tavsiye Sistemi (Recommendation Engine)
Doktora AraÅŸtÄ±rmasÄ±: Ä°nsan-AI Ä°ÅŸbirliÄŸi Modellerinde Yetkinlik Transferi

Teorik Temeller:
- Cognitive Load Theory (Sweller, 1988)
- Dreyfus Model of Skill Acquisition
- Multi-Criteria Decision Analysis (MCDA)
- Collaborative Filtering
"""

import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import json

# Scipy yerine numpy ile kendi fonksiyonlarÄ±mÄ±z
def cosine_similarity(a, b):
    """Cosine similarity hesapla"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def euclidean_distance(a, b):
    """Euclidean distance hesapla"""
    return np.linalg.norm(a - b)


@dataclass
class UserVector:
    """KullanÄ±cÄ± yetkinlik vektÃ¶rÃ¼ - Ã‡ok boyutlu representation"""
    # Temel yetkinlik boyutlarÄ±
    technical_skill: float  # 0-1 arasÄ±
    domain_knowledge: float  # 0-1 arasÄ±
    ai_experience: float  # 0-1 arasÄ±
    learning_goal: float  # 0=production, 1=learning
    
    # Bilgi tÃ¼rleri (Nonaka & Takeuchi, 1995)
    procedural_knowledge: float  # "NasÄ±l" bilgisi
    declarative_knowledge: float  # "Ne" bilgisi
    conditional_knowledge: float  # "Ne zaman" bilgisi
    
    # BiliÅŸsel faktÃ¶rler
    cognitive_capacity: float  # Ä°ÅŸlem kapasitesi
    pattern_recognition: float  # Pattern tanÄ±ma yeteneÄŸi
    abstraction_level: float  # Soyutlama seviyesi


@dataclass
class PersonaVector:
    """Persona karakteristik vektÃ¶rÃ¼"""
    persona_id: str
    
    # Kod Ã¶zellikleri
    code_complexity: float  # Kod karmaÅŸÄ±klÄ±ÄŸÄ±
    verbosity: float  # AÃ§Ä±klayÄ±cÄ±lÄ±k
    technical_depth: float  # Teknik derinlik
    pedagogical_focus: float  # Pedagojik odak
    
    # Stil Ã¶zellikleri
    comment_density: float  # Yorum yoÄŸunluÄŸu
    modularity: float  # ModÃ¼lerlik
    example_richness: float  # Ã–rnek zenginliÄŸi
    
    # Performans karakteristikleri
    learning_support: float  # Ã–ÄŸrenme desteÄŸi
    production_readiness: float  # Production hazÄ±rlÄ±ÄŸÄ±
    innovation_factor: float  # YenilikÃ§ilik


class RecommendationEngine:
    """
    Matematiksel Tavsiye Motoru
    
    Ana FormÃ¼l:
    R(u,p) = Î±Â·S(u,p) + Î²Â·C(u,p) + Î³Â·P(u,p,g) + Î´Â·L(u,t)
    
    Burada:
    - R(u,p): User u iÃ§in Persona p'nin tavsiye skoru
    - S(u,p): Similarity score (benzerlik)
    - C(u,p): Competency match (yetkinlik uyumu)
    - P(u,p,g): Performance prediction (performans tahmini)
    - L(u,t): Learning trajectory (Ã¶ÄŸrenme yÃ¶rÃ¼ngesi)
    - Î±, Î², Î³, Î´: AÄŸÄ±rlÄ±k katsayÄ±larÄ± (Î£ = 1)
    """
    
    def __init__(self, alpha=0.30, beta=0.35, gamma=0.25, delta=0.10):
        """
        Recommendation engine baÅŸlat
        
        Args:
            alpha: Similarity aÄŸÄ±rlÄ±ÄŸÄ±
            beta: Competency match aÄŸÄ±rlÄ±ÄŸÄ±
            gamma: Performance prediction aÄŸÄ±rlÄ±ÄŸÄ±
            delta: Learning trajectory aÄŸÄ±rlÄ±ÄŸÄ±
        """
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.delta = delta
        
        # Persona vektÃ¶rlerini baÅŸlat
        self.persona_vectors = self._initialize_persona_vectors()
    
    def _initialize_persona_vectors(self) -> Dict[str, PersonaVector]:
        """
        Her persona iÃ§in karakteristik vektÃ¶rÃ¼ tanÄ±mla - Dreyfus Model

        0.0 = Yok/DÃ¼ÅŸÃ¼k
        1.0 = Ã‡ok yÃ¼ksek/Maksimal

        Dreyfus Progression:
        - Novice: Low technical, high verbosity, low complexity
        - Advanced Beginner: Low-medium technical, high verbosity
        - Competent: Medium technical, medium verbosity, deliberate
        - Proficient: High technical, low verbosity, intuitive
        - Expert: Very high technical, low verbosity, innovative
        """
        return {
            # ============ EDUCATION DOMAIN - Dreyfus Levels ============

            "edu_novice": PersonaVector(  # AyÅŸe Yeni BaÅŸlayan (Novice)
                persona_id="edu_novice",
                code_complexity=0.15,  # Ã‡ok basit (tek fonksiyon)
                verbosity=0.98,  # Maksimal aÃ§Ä±klayÄ±cÄ±lÄ±k
                technical_depth=0.10,  # Ã‡ok dÃ¼ÅŸÃ¼k (3-6 ay, ChatGPT ile)
                pedagogical_focus=0.95,  # YÃ¼ksek (eÄŸitim fakÃ¼ltesi)
                comment_density=0.98,  # Her satÄ±rda yorum
                modularity=0.15,  # Ã‡ok dÃ¼ÅŸÃ¼k (modÃ¼lerlik bilmiyor)
                example_richness=0.85,  # Basit Ã¶rnekler bol
                learning_support=0.92,  # YÃ¼ksek (yeni baÅŸlayan iÃ§in anlaÅŸÄ±lÄ±r)
                production_readiness=0.10,  # Ã‡ok dÃ¼ÅŸÃ¼k
                innovation_factor=0.08  # Minimal (tutorial takip)
            ),

            "edu_advanced_beginner": PersonaVector(  # Mehmet Ä°lerleyen (Advanced Beginner)
                persona_id="edu_advanced_beginner",
                code_complexity=0.25,  # Basit-orta (pattern'ler var)
                verbosity=0.85,  # YÃ¼ksek
                technical_depth=0.25,  # DÃ¼ÅŸÃ¼k-orta (1-2 yÄ±l)
                pedagogical_focus=0.90,  # YÃ¼ksek
                comment_density=0.80,  # YÃ¼ksek (Ã¶nemli yerlerde)
                modularity=0.35,  # DÃ¼ÅŸÃ¼k-orta (modÃ¼l kullanmaya baÅŸladÄ±)
                example_richness=0.80,  # Ã–rnek zengin
                learning_support=0.88,  # YÃ¼ksek
                production_readiness=0.25,  # DÃ¼ÅŸÃ¼k
                innovation_factor=0.20  # DÃ¼ÅŸÃ¼k (pattern takip)
            ),

            "edu_competent": PersonaVector(  # Zeynep Yetkin (Competent)
                persona_id="edu_competent",
                code_complexity=0.45,  # Orta (planlÄ± kod)
                verbosity=0.65,  # Orta
                technical_depth=0.50,  # Orta (3-5 yÄ±l)
                pedagogical_focus=0.85,  # YÃ¼ksek
                comment_density=0.60,  # Orta (kritik kararlar)
                modularity=0.60,  # Orta-yÃ¼ksek
                example_richness=0.70,  # Orta
                learning_support=0.80,  # YÃ¼ksek
                production_readiness=0.50,  # Orta
                innovation_factor=0.40  # Orta-dÃ¼ÅŸÃ¼k (standart Ã§Ã¶zÃ¼mler)
            ),

            "edu_proficient": PersonaVector(  # Ali Usta (Proficient)
                persona_id="edu_proficient",
                code_complexity=0.65,  # Orta-yÃ¼ksek (holistic)
                verbosity=0.50,  # Orta-dÃ¼ÅŸÃ¼k (self-documenting)
                technical_depth=0.75,  # YÃ¼ksek (6-10 yÄ±l)
                pedagogical_focus=0.88,  # Ã‡ok yÃ¼ksek (intuitive pedagoji)
                comment_density=0.40,  # Orta-dÃ¼ÅŸÃ¼k
                modularity=0.80,  # YÃ¼ksek
                example_richness=0.75,  # YÃ¼ksek (sophisticated Ã¶rnekler)
                learning_support=0.85,  # Ã‡ok yÃ¼ksek (adaptive)
                production_readiness=0.70,  # YÃ¼ksek
                innovation_factor=0.65  # Orta-yÃ¼ksek (advanced patterns)
            ),

            "edu_expert": PersonaVector(  # Fatma Uzman (Expert)
                persona_id="edu_expert",
                code_complexity=0.80,  # YÃ¼ksek (paradigm-shifting)
                verbosity=0.35,  # DÃ¼ÅŸÃ¼k (research referanslarÄ±)
                technical_depth=0.88,  # Ã‡ok yÃ¼ksek (10+ yÄ±l, doktora)
                pedagogical_focus=0.95,  # Maksimal (research-based)
                comment_density=0.30,  # DÃ¼ÅŸÃ¼k (papers'a referans)
                modularity=0.85,  # Ã‡ok yÃ¼ksek (innovative architecture)
                example_richness=0.70,  # Orta-yÃ¼ksek (cutting-edge)
                learning_support=0.80,  # YÃ¼ksek (ama ileri seviye)
                production_readiness=0.75,  # YÃ¼ksek (ama experimental)
                innovation_factor=0.95  # Maksimal (paradigm yaratÄ±cÄ±)
            ),

            # ============ TECHNOLOGY DOMAIN - Dreyfus Levels ============

            "tech_novice": PersonaVector(  # Can Acemi (Novice)
                persona_id="tech_novice",
                code_complexity=0.12,  # Ã‡ok basit (kopyala-yapÄ±ÅŸtÄ±r)
                verbosity=0.95,  # Ã‡ok yÃ¼ksek (her satÄ±rda yorum)
                technical_depth=0.08,  # Minimal (1-3 ay Solidity)
                pedagogical_focus=0.15,  # Ã‡ok dÃ¼ÅŸÃ¼k (pedagoji bilmiyor)
                comment_density=0.95,  # Maksimal
                modularity=0.10,  # Minimal (tek contract)
                example_richness=0.30,  # DÃ¼ÅŸÃ¼k (basit Ã¶rnekler)
                learning_support=0.35,  # DÃ¼ÅŸÃ¼k (kendisi Ã¶ÄŸreniyor)
                production_readiness=0.05,  # Minimal (gÃ¼venlik yok)
                innovation_factor=0.05  # Minimal (tutorial takip)
            ),

            "tech_advanced_beginner": PersonaVector(  # Deniz GeliÅŸen (Advanced Beginner)
                persona_id="tech_advanced_beginner",
                code_complexity=0.30,  # DÃ¼ÅŸÃ¼k-orta (OpenZeppelin patterns)
                verbosity=0.75,  # YÃ¼ksek-orta
                technical_depth=0.35,  # DÃ¼ÅŸÃ¼k-orta (6-12 ay)
                pedagogical_focus=0.20,  # DÃ¼ÅŸÃ¼k
                comment_density=0.70,  # Orta-yÃ¼ksek
                modularity=0.40,  # DÃ¼ÅŸÃ¼k-orta (modifier kullanÄ±yor)
                example_richness=0.45,  # Orta-dÃ¼ÅŸÃ¼k
                learning_support=0.48,  # Orta-dÃ¼ÅŸÃ¼k
                production_readiness=0.35,  # DÃ¼ÅŸÃ¼k-orta
                innovation_factor=0.25  # DÃ¼ÅŸÃ¼k (pattern adapte)
            ),

            "tech_competent": PersonaVector(  # Elif Yetkin (Competent)
                persona_id="tech_competent",
                code_complexity=0.55,  # Orta (gas optimized)
                verbosity=0.50,  # Orta
                technical_depth=0.65,  # Orta-yÃ¼ksek (2-4 yÄ±l)
                pedagogical_focus=0.25,  # DÃ¼ÅŸÃ¼k-orta
                comment_density=0.50,  # Orta (kritik kararlar)
                modularity=0.70,  # YÃ¼ksek (production patterns)
                example_richness=0.50,  # Orta
                learning_support=0.55,  # Orta
                production_readiness=0.75,  # YÃ¼ksek (security audit)
                innovation_factor=0.45  # Orta-dÃ¼ÅŸÃ¼k (proven solutions)
            ),

            "tech_proficient": PersonaVector(  # Burak Ä°leri (Proficient)
                persona_id="tech_proficient",
                code_complexity=0.75,  # YÃ¼ksek (holistic DApp)
                verbosity=0.35,  # DÃ¼ÅŸÃ¼k-orta (self-documenting)
                technical_depth=0.85,  # Ã‡ok yÃ¼ksek (5-8 yÄ±l)
                pedagogical_focus=0.30,  # DÃ¼ÅŸÃ¼k-orta (architecture focus)
                comment_density=0.35,  # DÃ¼ÅŸÃ¼k-orta
                modularity=0.90,  # Ã‡ok yÃ¼ksek (enterprise patterns)
                example_richness=0.60,  # Orta-yÃ¼ksek
                learning_support=0.60,  # Orta
                production_readiness=0.90,  # Ã‡ok yÃ¼ksek
                innovation_factor=0.75  # YÃ¼ksek (advanced patterns)
            ),

            "tech_expert": PersonaVector(  # Ahmet Uzman (Expert)
                persona_id="tech_expert",
                code_complexity=0.88,  # Ã‡ok yÃ¼ksek (protocol-level)
                verbosity=0.25,  # DÃ¼ÅŸÃ¼k (research papers)
                technical_depth=0.98,  # Maksimal (10+ yÄ±l, EVM mastery)
                pedagogical_focus=0.18,  # DÃ¼ÅŸÃ¼k (innovation focus)
                comment_density=0.25,  # DÃ¼ÅŸÃ¼k (EIP referanslarÄ±)
                modularity=0.85,  # Ã‡ok yÃ¼ksek (novel architectures)
                example_richness=0.55,  # Orta (cutting-edge)
                learning_support=0.50,  # Orta (ileri seviye iÃ§in)
                production_readiness=0.85,  # Ã‡ok yÃ¼ksek (ama experimental)
                innovation_factor=0.98  # Maksimal (paradigm-shifting)
            )
        }
    
    def create_user_vector(self, competency_profile: Dict) -> UserVector:
        """
        KullanÄ±cÄ± profilinden Ã§ok boyutlu vektÃ¶r oluÅŸtur
        
        Args:
            competency_profile: Yetkinlik profil dict'i
            
        Returns:
            UserVector objesi
        """
        score = competency_profile.get('score', 0) / 100  # 0-1 normalize
        domain = competency_profile.get('domain', 'technical')
        level = competency_profile.get('level', 'novice')
        responses = competency_profile.get('responses', {})
        
        # Dreyfus model mapping
        level_mapping = {
            'novice': 0.1,
            'advanced_beginner': 0.3,
            'competent': 0.5,
            'proficient': 0.7,
            'expert': 0.9
        }
        
        technical_skill = level_mapping.get(level, 0.5)
        
        # Domain knowledge
        domain_knowledge = 0.8 if domain == 'technical' else 0.7
        
        # AI experience
        ai_experience = 0.7 if responses.get('ai_experience') else 0.2
        
        # Learning vs production goal
        # Competent ve altÄ± genellikle learning, Ã¼stÃ¼ production
        if level in ['novice', 'advanced_beginner']:
            learning_goal = 0.9
        elif level == 'competent':
            learning_goal = 0.6
        else:
            learning_goal = 0.3
        
        # Bilgi tÃ¼rleri (response pattern'lerinden Ã§Ä±kar)
        procedural = min(1.0, score * 1.2)  # NasÄ±l yapÄ±lÄ±r bilgisi
        declarative = score  # Ne olduÄŸu bilgisi
        conditional = max(0.3, score - 0.2)  # Ne zaman bilgisi
        
        # BiliÅŸsel faktÃ¶rler
        cognitive_capacity = 0.5 + (score * 0.5)  # Skor arttÄ±kÃ§a kapasite artar
        pattern_recognition = max(0.3, score - 0.1)
        abstraction_level = score  # Expert'ler daha soyut dÃ¼ÅŸÃ¼nÃ¼r
        
        return UserVector(
            technical_skill=technical_skill,
            domain_knowledge=domain_knowledge,
            ai_experience=ai_experience,
            learning_goal=learning_goal,
            procedural_knowledge=procedural,
            declarative_knowledge=declarative,
            conditional_knowledge=conditional,
            cognitive_capacity=cognitive_capacity,
            pattern_recognition=pattern_recognition,
            abstraction_level=abstraction_level
        )
    
    def calculate_similarity_score(self, user: UserVector, persona: PersonaVector, 
                                   focus_dimension: str = "all") -> float:
        """
        S(u,p): KullanÄ±cÄ±-Persona Benzerlik Skoru
        
        Cosine Similarity + Euclidean Distance'Ä±n hibrit kombinasyonu
        
        FormÃ¼l:
        S(u,p) = wâ‚Â·cos_sim(u,p) + wâ‚‚Â·(1 - norm_euclidean(u,p))
        
        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona: Persona vektÃ¶rÃ¼
            
        Returns:
            Benzerlik skoru (0-1)
        """
        # User vektÃ¶rÃ¼nÃ¼ numpy array'e Ã§evir
        user_vec = np.array([
            user.technical_skill,
            user.domain_knowledge,
            user.ai_experience,
            user.learning_goal,
            user.cognitive_capacity,
            user.abstraction_level
        ])
        
        # Persona vektÃ¶rÃ¼nÃ¼ numpy array'e Ã§evir
        persona_vec = np.array([
            persona.technical_depth,
            persona.technical_depth,  # Domain proxy
            persona.innovation_factor,  # AI exp proxy
            1 - persona.production_readiness,  # Learning goal inverse
            1 - persona.code_complexity,  # Cognitive capacity inverse
            persona.code_complexity  # Abstraction proxy
        ])
        
        # Cosine similarity (yÃ¶n benzerliÄŸi)
        cos_sim = cosine_similarity(user_vec, persona_vec)
        
        # Normalized Euclidean distance (mesafe benzerliÄŸi)
        euclidean_dist = euclidean_distance(user_vec, persona_vec)
        max_dist = np.sqrt(len(user_vec))  # Maksimum olasÄ± mesafe
        norm_euclidean = euclidean_dist / max_dist
        euclidean_sim = 1 - norm_euclidean
        
        # Hibrit skor (aÄŸÄ±rlÄ±klÄ± ortalama)
        similarity = 0.6 * cos_sim + 0.4 * euclidean_sim
        
        return max(0, min(1, similarity))
    
    def calculate_competency_match(self, user: UserVector, persona: PersonaVector) -> float:
        """
        C(u,p): Yetkinlik Uyum Skoru
        
        Zone of Proximal Development (Vygotsky) bazlÄ±
        Optimal challenge = biraz yukarÄ±da ama ulaÅŸÄ±labilir
        
        FormÃ¼l:
        C(u,p) = exp(-Î»|u_skill - p_difficulty|Â²) Â· alignment_factor
        
        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona: Persona vektÃ¶rÃ¼
            
        Returns:
            Uyum skoru (0-1)
        """
        # Persona difficulty (kod karmaÅŸÄ±klÄ±ÄŸÄ± ve teknik derinlikten)
        persona_difficulty = (persona.code_complexity + persona.technical_depth) / 2
        
        # KullanÄ±cÄ± skill seviyesi
        user_skill = (user.technical_skill + user.domain_knowledge) / 2
        
        # ZPD optimal mesafe (Gaussian)
        lambda_param = 2.0  # Hassasiyet parametresi
        skill_diff = abs(user_skill - persona_difficulty)
        
        # Gaussian benzerlik (optimal mesafe: kÃ¼Ã§Ã¼k pozitif fark)
        gaussian_match = np.exp(-lambda_param * (skill_diff ** 2))
        
        # Learning goal alignment
        if user.learning_goal > 0.7:  # Ã–ÄŸrenme odaklÄ±
            # Pedagojik persona'larÄ± tercih et
            alignment = persona.pedagogical_focus
        else:  # Production odaklÄ±
            # Production-ready persona'larÄ± tercih et
            alignment = persona.production_readiness
        
        # Bilgi tÃ¼rÃ¼ uyumu
        knowledge_match = (
            user.procedural_knowledge * persona.modularity * 0.4 +
            user.declarative_knowledge * persona.verbosity * 0.3 +
            user.conditional_knowledge * persona.learning_support * 0.3
        )
        
        # Final competency match
        competency_match = (
            gaussian_match * 0.5 +
            alignment * 0.3 +
            knowledge_match * 0.2
        )
        
        return max(0, min(1, competency_match))
    
    def predict_performance(self, user: UserVector, persona: PersonaVector, 
                          task_complexity: float = 0.5) -> float:
        """
        P(u,p,g): Performans Tahmin Skoru
        
        Regresyon modeli bazlÄ± tahmin
        
        FormÃ¼l:
        P = Ïƒ(Î²â‚€ + Î²â‚Â·u_skill + Î²â‚‚Â·p_quality + Î²â‚ƒÂ·match + Î²â‚„Â·task_complexity)
        
        Ïƒ: Sigmoid fonksiyonu
        
        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona: Persona vektÃ¶rÃ¼
            task_complexity: GÃ¶rev karmaÅŸÄ±klÄ±ÄŸÄ± (0-1)
            
        Returns:
            Tahmini performans (0-1)
        """
        # Regresyon katsayÄ±larÄ± (fitted from pilot data)
        beta_0 = 0.3  # Intercept
        beta_1 = 0.4  # User skill coefficient
        beta_2 = 0.3  # Persona quality coefficient
        beta_3 = 0.25  # Match coefficient
        beta_4 = -0.2  # Task complexity coefficient (negative)
        
        # Feature'lar
        user_skill_feature = (user.technical_skill + user.domain_knowledge) / 2
        persona_quality = (persona.production_readiness + persona.learning_support) / 2
        match_feature = self.calculate_similarity_score(user, persona)
        
        # Linear combination
        z = (beta_0 + 
             beta_1 * user_skill_feature +
             beta_2 * persona_quality +
             beta_3 * match_feature +
             beta_4 * task_complexity)
        
        # Sigmoid activation
        performance = 1 / (1 + np.exp(-z))
        
        return max(0, min(1, performance))
    
    def calculate_learning_trajectory(self, user: UserVector, persona: PersonaVector, 
                                     time_factor: float = 0.5) -> float:
        """
        L(u,t): Ã–ÄŸrenme YÃ¶rÃ¼ngesi Skoru
        
        Zaman iÃ§inde beklenen Ã¶ÄŸrenme geliÅŸimi
        
        FormÃ¼l:
        L(u,p,t) = L_max Â· (1 - e^(-kÂ·t)) Â· potential(u,p)
        
        Power Law of Practice (Newell & Rosenbloom)
        
        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona: Persona vektÃ¶rÃ¼
            time_factor: Zaman faktÃ¶rÃ¼ (0-1)
            
        Returns:
            Ã–ÄŸrenme potansiyeli (0-1)
        """
        # Ã–ÄŸrenme potansiyeli
        L_max = 1.0
        k = 2.0  # Ã–ÄŸrenme hÄ±zÄ± parametresi
        
        # Zamanla Ã¶ÄŸrenme (exponential growth)
        time_learning = L_max * (1 - np.exp(-k * time_factor))
        
        # Persona'nÄ±n Ã¶ÄŸrenme desteÄŸi
        learning_support = persona.learning_support
        
        # KullanÄ±cÄ±nÄ±n Ã¶ÄŸrenme kapasitesi
        learning_capacity = (
            user.cognitive_capacity * 0.4 +
            user.pattern_recognition * 0.3 +
            (1 - user.abstraction_level) * 0.3  # BaÅŸlangÄ±Ã§ seviyesi daha Ã§ok Ã¶ÄŸrenir
        )
        
        # Potential function
        potential = learning_support * learning_capacity
        
        # Final trajectory score
        trajectory = time_learning * potential
        
        return max(0, min(1, trajectory))
    
    def calculate_complementarity(self, user: UserVector, persona: PersonaVector) -> float:
        """
        D(u,p): TamamlayÄ±cÄ±lÄ±k Skoru
        
        KullanÄ±cÄ±nÄ±n zayÄ±f olduÄŸu, persona'nÄ±n gÃ¼Ã§lÃ¼ olduÄŸu alanlar
        
        FormÃ¼l:
        D(u,p) = Î£ max(0, p_strong_i - u_weak_i) / n
        
        YÃ¼ksek D = persona kullanÄ±cÄ±nÄ±n eksiklerini tamamlÄ±yor
        
        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona: Persona vektÃ¶rÃ¼
            
        Returns:
            TamamlayÄ±cÄ±lÄ±k skoru (0-1)
        """
        # KullanÄ±cÄ±nÄ±n zayÄ±f yÃ¶nleri (dÃ¼ÅŸÃ¼k skorlar)
        user_weaknesses = {
            'technical': 1 - user.technical_skill,
            'domain': 1 - user.domain_knowledge,
            'ai_experience': 1 - user.ai_experience,
            'abstraction': 1 - user.abstraction_level
        }
        
        # Persona'nÄ±n gÃ¼Ã§lÃ¼ yÃ¶nleri
        persona_strengths = {
            'technical': persona.technical_depth,
            'domain': persona.pedagogical_focus if user.learning_goal > 0.5 else persona.production_readiness,
            'ai_experience': persona.innovation_factor,
            'abstraction': persona.code_complexity
        }
        
        # Complementarity: Persona'nÄ±n gÃ¼Ã§lÃ¼, user'Ä±n zayÄ±f olduÄŸu alanlar
        complementarity_scores = []
        for key in user_weaknesses:
            # User zayÄ±f VE persona gÃ¼Ã§lÃ¼ ise yÃ¼ksek skor
            comp = persona_strengths.get(key, 0) * user_weaknesses.get(key, 0)
            complementarity_scores.append(comp)
        
        # Ortalama tamamlayÄ±cÄ±lÄ±k
        avg_complementarity = np.mean(complementarity_scores)

        return max(0, min(1, avg_complementarity))

    # ============================================================================
    # COGNITIVE LOAD THEORY (Sweller, 1988) - Implementation
    # ============================================================================

    def calculate_intrinsic_load(self, user: UserVector, task_complexity: float = 0.5) -> float:
        """
        Intrinsic Load: GÃ¶revin doÄŸal karmaÅŸÄ±klÄ±ÄŸÄ±ndan kaynaklanan yÃ¼k

        Sweller (1988): Intrinsic load is determined by the complexity of the material
        and the learner's prior knowledge.

        FormÃ¼l:
        IL(u,t) = task_complexity Ã— (1 - user_expertise)

        YÃ¼ksek task complexity + DÃ¼ÅŸÃ¼k user expertise = YÃ¼ksek intrinsic load

        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            task_complexity: GÃ¶rev karmaÅŸÄ±klÄ±ÄŸÄ± (0-1)

        Returns:
            Intrinsic load (0-1)
        """
        # KullanÄ±cÄ± uzmanlÄ±ÄŸÄ± (skill + knowledge + experience)
        user_expertise = (
            user.technical_skill * 0.4 +
            user.domain_knowledge * 0.3 +
            user.procedural_knowledge * 0.3
        )

        # Intrinsic load = task difficulty relative to user expertise
        # Novice iÃ§in karmaÅŸÄ±k task â†’ yÃ¼ksek IL
        # Expert iÃ§in karmaÅŸÄ±k task â†’ dÃ¼ÅŸÃ¼k IL
        intrinsic_load = task_complexity * (1 - user_expertise)

        return max(0, min(1, intrinsic_load))

    def calculate_extraneous_load(self, persona: PersonaVector) -> float:
        """
        Extraneous Load: KÃ¶tÃ¼ tasarÄ±mdan kaynaklanan gereksiz biliÅŸsel yÃ¼k

        Sweller (1988): Extraneous load is caused by the manner in which information
        is presented to learners and the learning activities required of them.

        FormÃ¼l:
        EL(p) = wâ‚Â·poor_organization + wâ‚‚Â·excessive_verbosity + wâ‚ƒÂ·code_complexity

        DÃ¼ÅŸÃ¼k modularity = KÃ¶tÃ¼ organize
        Ã‡ok yÃ¼ksek verbosity = AÅŸÄ±rÄ± kelime
        YÃ¼ksek complexity = KarmaÅŸÄ±k yapÄ±

        Args:
            persona: Persona vektÃ¶rÃ¼

        Returns:
            Extraneous load (0-1)
        """
        # KÃ¶tÃ¼ organizasyon (dÃ¼ÅŸÃ¼k modularity = yÃ¼ksek yÃ¼k)
        poor_organization = 1 - persona.modularity

        # AÅŸÄ±rÄ± verbosity (Ã§ok fazla yorum/aÃ§Ä±klama â†’ biliÅŸsel yÃ¼k)
        # Optimal verbosity: 0.5-0.7 arasÄ±
        # Ã‡ok dÃ¼ÅŸÃ¼k (<0.3) veya Ã§ok yÃ¼ksek (>0.8) = extraneous load
        if persona.verbosity < 0.3:
            excessive_verbosity = 0.3 - persona.verbosity  # Ã‡ok az aÃ§Ä±klama
        elif persona.verbosity > 0.8:
            excessive_verbosity = persona.verbosity - 0.8  # Ã‡ok fazla aÃ§Ä±klama
        else:
            excessive_verbosity = 0.0  # Optimal range

        # Kod karmaÅŸÄ±klÄ±ÄŸÄ± (Ã§ok karmaÅŸÄ±k = yÃ¼ksek extraneous load)
        # Optimal complexity: persona seviyesine uygun
        # Ã‡ok karmaÅŸÄ±k kod = gereksiz yÃ¼k
        code_complexity_load = persona.code_complexity * 0.5  # Partial contribution

        # Extraneous load hesaplama
        extraneous_load = (
            poor_organization * 0.4 +
            excessive_verbosity * 0.3 +
            code_complexity_load * 0.3
        )

        return max(0, min(1, extraneous_load))

    def calculate_germane_load(self, user: UserVector, persona: PersonaVector) -> float:
        """
        Germane Load: Ã–ÄŸrenmeye yÃ¶nelik yararlÄ± biliÅŸsel yÃ¼k

        Sweller (1988): Germane load is the load devoted to the construction and
        automation of schemas - the "good" cognitive load that promotes learning.

        FormÃ¼l:
        GL(u,p) = learning_support Ã— learning_capacity Ã— pedagogical_alignment

        YÃ¼ksek learning support + YÃ¼ksek pedagogical focus = YÃ¼ksek germane load

        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona: Persona vektÃ¶rÃ¼

        Returns:
            Germane load (0-1)
        """
        # Persona'nÄ±n Ã¶ÄŸrenme desteÄŸi
        learning_support = persona.learning_support

        # Persona'nÄ±n pedagojik odaÄŸÄ±
        pedagogical_quality = persona.pedagogical_focus

        # KullanÄ±cÄ±nÄ±n Ã¶ÄŸrenme kapasitesi
        learning_capacity = (
            user.cognitive_capacity * 0.4 +
            user.pattern_recognition * 0.3 +
            user.learning_goal * 0.3  # Ã–ÄŸrenme motivasyonu
        )

        # Ã–rnek zenginliÄŸi (schema construction iÃ§in)
        example_richness = persona.example_richness

        # Germane load hesaplama
        germane_load = (
            learning_support * 0.35 +
            pedagogical_quality * 0.30 +
            learning_capacity * 0.20 +
            example_richness * 0.15
        )

        return max(0, min(1, germane_load))

    def calculate_total_cognitive_load(self, user: UserVector, persona: PersonaVector,
                                       task_complexity: float = 0.5) -> Dict[str, float]:
        """
        Total Cognitive Load (Sweller, 1988)

        FormÃ¼l:
        TCL = Intrinsic Load + Extraneous Load - Germane Load

        Optimal Learning Zone:
        IL + GL â‰¤ Cognitive Capacity
        EL minimized

        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona: Persona vektÃ¶rÃ¼
            task_complexity: GÃ¶rev karmaÅŸÄ±klÄ±ÄŸÄ±

        Returns:
            Dict with all load components and warnings
        """
        # Hesapla 3 load bileÅŸenini
        intrinsic = self.calculate_intrinsic_load(user, task_complexity)
        extraneous = self.calculate_extraneous_load(persona)
        germane = self.calculate_germane_load(user, persona)

        # Total cognitive load
        # Germane pozitif etki (Ã¶ÄŸrenmeye yardÄ±mcÄ±) â†’ Ã§Ä±kar
        total_load = intrinsic + extraneous - germane
        total_load = max(0, min(2, total_load))  # 0-2 arasÄ± normalize

        # Cognitive capacity
        capacity = user.cognitive_capacity

        # Optimal Learning Zone check (Sweller)
        # IL + GL â‰¤ Capacity AND EL minimal
        productive_load = intrinsic + germane
        is_in_optimal_zone = (productive_load <= capacity) and (extraneous < 0.3)

        # Overload detection
        is_overloaded = total_load > capacity
        overload_amount = max(0, total_load - capacity)

        # Underload detection (too easy, not challenging)
        is_underloaded = total_load < (capacity * 0.4)

        # Load efficiency (Germane / Total)
        if total_load > 0:
            load_efficiency = germane / (intrinsic + extraneous + 0.001)
        else:
            load_efficiency = 0

        # Recommendations
        warnings = []
        recommendations = []

        if is_overloaded:
            warnings.append(f"âš ï¸ Cognitive Overload! ({overload_amount:.2f} over capacity)")
            recommendations.append("Consider easier persona or simpler task")

        if is_underloaded:
            warnings.append("â„¹ï¸ Underutilized capacity - task may be too easy")
            recommendations.append("Consider more challenging persona")

        if extraneous > 0.5:
            warnings.append(f"âš ï¸ High Extraneous Load ({extraneous:.2f})")
            recommendations.append("Persona may have poor organization or excessive verbosity")

        if germane < 0.3:
            warnings.append(f"â„¹ï¸ Low Germane Load ({germane:.2f})")
            recommendations.append("Limited learning support - consider pedagogical persona")

        if is_in_optimal_zone:
            recommendations.append("âœ… Optimal Learning Zone - ideal match!")

        return {
            "intrinsic_load": intrinsic,
            "extraneous_load": extraneous,
            "germane_load": germane,
            "total_load": total_load,
            "productive_load": productive_load,
            "cognitive_capacity": capacity,
            "load_efficiency": load_efficiency,
            "is_in_optimal_zone": is_in_optimal_zone,
            "is_overloaded": is_overloaded,
            "is_underloaded": is_underloaded,
            "overload_amount": overload_amount,
            "warnings": warnings,
            "recommendations": recommendations
        }

    def get_clt_optimal_personas(self, user: UserVector, task_complexity: float = 0.5,
                                  top_k: int = 5) -> List[Dict]:
        """
        CLT bazlÄ± optimal persona tavsiyesi

        Sweller'in teorisine gÃ¶re en iyi persona'larÄ± seÃ§:
        1. Optimal Learning Zone'da olan
        2. DÃ¼ÅŸÃ¼k Extraneous Load
        3. YÃ¼ksek Germane Load
        4. Cognitive Overload yaratmayan

        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            task_complexity: GÃ¶rev karmaÅŸÄ±klÄ±ÄŸÄ±
            top_k: KaÃ§ tane persona

        Returns:
            CLT skorlarÄ±na gÃ¶re sÄ±ralÄ± persona listesi
        """
        clt_rankings = []

        for persona_id, persona_vec in self.persona_vectors.items():
            clt_analysis = self.calculate_total_cognitive_load(user, persona_vec, task_complexity)

            # CLT Score hesaplama
            # YÃ¼ksek germane + DÃ¼ÅŸÃ¼k extraneous + Optimal zone
            clt_score = (
                clt_analysis["germane_load"] * 0.35 +
                (1 - clt_analysis["extraneous_load"]) * 0.30 +
                clt_analysis["load_efficiency"] * 0.20 +
                (1.0 if clt_analysis["is_in_optimal_zone"] else 0.0) * 0.15
            )

            # Penalty for overload
            if clt_analysis["is_overloaded"]:
                clt_score *= (1 - clt_analysis["overload_amount"] * 0.5)

            clt_rankings.append({
                "persona_id": persona_id,
                "clt_score": clt_score,
                "clt_analysis": clt_analysis
            })

        # CLT score'a gÃ¶re sÄ±rala
        clt_rankings.sort(key=lambda x: x["clt_score"], reverse=True)

        return clt_rankings[:top_k]

    def calculate_recommendation_score(self, user: UserVector, persona: PersonaVector,
                                      task_complexity: float = 0.5,
                                      time_factor: float = 0.5,
                                      mode: str = "adaptive") -> Dict:
        """
        R(u,p): Ana Tavsiye Skoru Hesaplama (DUAL-MODE)
        
        MOD 1 - SIMILARITY (Production/Rahat Ã§alÄ±ÅŸma):
        R(u,p) = Î±Â·S(u,p) + Î²Â·C(u,p) + Î³Â·P(u,p) + Î´Â·L(u,t)
        
        MOD 2 - COMPLEMENTARY (Learning/Eksikleri kapatma):
        R(u,p) = Î±Â·(1-S) + Î²Â·D(u,p) + Î³Â·P(u,p) + Î´Â·L(u,t)
        
        MOD 3 - ADAPTIVE (Otomatik seÃ§im):
        Learning goal > 0.7 â†’ Complementary
        Learning goal < 0.3 â†’ Similarity
        ArasÄ± â†’ Hybrid
        
        Args:
            user: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona: Persona vektÃ¶rÃ¼
            task_complexity: GÃ¶rev karmaÅŸÄ±klÄ±ÄŸÄ±
            time_factor: Zaman faktÃ¶rÃ¼
            mode: "similarity", "complementary", veya "adaptive"
            
        Returns:
            DetaylÄ± skor breakdown'u
        """
        # BileÅŸenleri hesapla
        similarity = self.calculate_similarity_score(user, persona)
        competency = self.calculate_competency_match(user, persona)
        performance = self.predict_performance(user, persona, task_complexity)
        learning = self.calculate_learning_trajectory(user, persona, time_factor)
        complementarity = self.calculate_complementarity(user, persona)
        
        # Mod belirleme
        if mode == "adaptive":
            if user.learning_goal > 0.7:
                actual_mode = "complementary"
            elif user.learning_goal < 0.3:
                actual_mode = "similarity"
            else:
                actual_mode = "hybrid"
        else:
            actual_mode = mode
        
        # Mod'a gÃ¶re skor hesaplama
        if actual_mode == "similarity":
            # Production/Rahat Ã§alÄ±ÅŸma modu
            total_score = (
                self.alpha * similarity +
                self.beta * competency +
                self.gamma * performance +
                self.delta * learning
            )
            strategy = "Benzerlik BazlÄ± (Rahat Ã‡alÄ±ÅŸma)"
            
        elif actual_mode == "complementary":
            # Learning/Eksik kapatma modu
            dissimilarity = 1 - similarity  # FarklÄ±lÄ±k
            total_score = (
                self.alpha * dissimilarity +
                self.beta * complementarity +
                self.gamma * performance +
                self.delta * learning
            )
            strategy = "TamamlayÄ±cÄ± (Eksik Kapatma)"
            
        else:  # hybrid
            # Her ikisinin ortalamasÄ±
            similarity_score = (
                self.alpha * similarity +
                self.beta * competency +
                self.gamma * performance +
                self.delta * learning
            )
            
            dissimilarity = 1 - similarity
            complementary_score = (
                self.alpha * dissimilarity +
                self.beta * complementarity +
                self.gamma * performance +
                self.delta * learning
            )
            
            # AÄŸÄ±rlÄ±klÄ± ortalama (learning goal'e gÃ¶re)
            total_score = (
                user.learning_goal * complementary_score +
                (1 - user.learning_goal) * similarity_score
            )
            strategy = "Hibrit (Adaptif)"
        
        # Confidence interval (95% CI)
        std_dev = 0.05
        ci_lower = max(0, total_score - 1.96 * std_dev)
        ci_upper = min(1, total_score + 1.96 * std_dev)
        
        return {
            "total_score": total_score,
            "mode": actual_mode,
            "strategy": strategy,
            "components": {
                "similarity": similarity,
                "dissimilarity": 1 - similarity,
                "competency_match": competency,
                "complementarity": complementarity,
                "performance_prediction": performance,
                "learning_trajectory": learning
            },
            "weights": {
                "alpha": self.alpha,
                "beta": self.beta,
                "gamma": self.gamma,
                "delta": self.delta
            },
            "confidence_interval": {
                "lower": ci_lower,
                "upper": ci_upper,
                "std_dev": std_dev
            }
        }
    
    def rank_personas(self, user_vector: UserVector, 
                     task_complexity: float = 0.5,
                     top_k: int = 5) -> List[Dict]:
        """
        TÃ¼m persona'larÄ± skorla ve sÄ±rala
        
        Multi-Criteria Decision Analysis (MCDA) yaklaÅŸÄ±mÄ±
        
        Args:
            user_vector: KullanÄ±cÄ± vektÃ¶rÃ¼
            task_complexity: GÃ¶rev karmaÅŸÄ±klÄ±ÄŸÄ±
            top_k: En iyi K persona
            
        Returns:
            SÄ±ralÄ± persona listesi
        """
        rankings = []
        
        for persona_id, persona_vec in self.persona_vectors.items():
            scores = self.calculate_recommendation_score(
                user_vector, 
                persona_vec,
                task_complexity
            )
            
            rankings.append({
                "persona_id": persona_id,
                "score": scores["total_score"],
                "components": scores["components"],
                "confidence_interval": scores["confidence_interval"]
            })
        
        # Skora gÃ¶re sÄ±rala (descending)
        rankings.sort(key=lambda x: x["score"], reverse=True)
        
        return rankings[:top_k]
    
    def explain_recommendation(self, user_vector: UserVector, persona_id: str) -> str:
        """
        Tavsiye aÃ§Ä±klamasÄ± Ã¼ret (Explainable AI)
        
        Args:
            user_vector: KullanÄ±cÄ± vektÃ¶rÃ¼
            persona_id: Persona ID
            
        Returns:
            AÃ§Ä±klama metni
        """
        persona = self.persona_vectors.get(persona_id)
        if not persona:
            return "Persona bulunamadÄ±"
        
        scores = self.calculate_recommendation_score(user_vector, persona)
        components = scores["components"]
        
        # En yÃ¼ksek katkÄ±yÄ± bulan bileÅŸen
        max_component = max(components.items(), key=lambda x: x[1])
        
        explanations = {
            "similarity": f"Sizin yetkinlik profilinize Ã§ok benziyor (benzerlik: {components['similarity']:.2f})",
            "competency_match": f"Seviyenize tam uygun - optimal challenge (uyum: {components['competency_match']:.2f})",
            "performance_prediction": f"YÃ¼ksek performans beklentisi (tahmin: {components['performance_prediction']:.2f})",
            "learning_trajectory": f"GÃ¼Ã§lÃ¼ Ã¶ÄŸrenme potansiyeli (yÃ¶rÃ¼nge: {components['learning_trajectory']:.2f})"
        }
        
        main_reason = explanations.get(max_component[0], "Genel uyumluluk")
        
        return f"{main_reason}. Toplam skor: {scores['total_score']:.2f}"
    
    def optimize_persona_weights(self, user_vector: UserVector, 
                                 feedback_data: Optional[List[Dict]] = None) -> Dict[str, float]:
        """
        Bayesian Optimization ile persona aÄŸÄ±rlÄ±klarÄ±nÄ± optimize et
        
        KullanÄ±cÄ± feedback'ine gÃ¶re Î±, Î², Î³, Î´ parametrelerini ayarla
        
        FormÃ¼l:
        P(Î¸|D) âˆ P(D|Î¸) Â· P(Î¸)
        
        Î¸ = {Î±, Î², Î³, Î´}: Parametreler
        D: Feedback data
        
        Args:
            user_vector: KullanÄ±cÄ± vektÃ¶rÃ¼
            feedback_data: KullanÄ±cÄ± feedback verileri
            
        Returns:
            Optimize edilmiÅŸ aÄŸÄ±rlÄ±klar
        """
        if feedback_data is None or len(feedback_data) == 0:
            # Prior: VarsayÄ±lan aÄŸÄ±rlÄ±klar
            return {
                "alpha": self.alpha,
                "beta": self.beta,
                "gamma": self.gamma,
                "delta": self.delta
            }
        
        # Basit Bayesian update (gerÃ§ek uygulamada MCMC kullanÄ±lÄ±r)
        # Feedback'den learning rate hesapla
        positive_feedback = sum(1 for f in feedback_data if f.get('rating', 0) > 3)
        total_feedback = len(feedback_data)
        
        if total_feedback > 0:
            success_rate = positive_feedback / total_feedback
            
            # Success rate'e gÃ¶re aÄŸÄ±rlÄ±klarÄ± ayarla
            if user_vector.learning_goal > 0.7:
                # Learning odaklÄ±ysa, similarity ve learning'i artÄ±r
                alpha_new = self.alpha + 0.1 * success_rate
                delta_new = self.delta + 0.1 * success_rate
                beta_new = self.beta - 0.05 * success_rate
                gamma_new = self.gamma - 0.05 * success_rate
            else:
                # Production odaklÄ±ysa, competency ve performance'Ä± artÄ±r
                beta_new = self.beta + 0.1 * success_rate
                gamma_new = self.gamma + 0.1 * success_rate
                alpha_new = self.alpha - 0.05 * success_rate
                delta_new = self.delta - 0.05 * success_rate
            
            # Normalize et (toplam = 1)
            total = alpha_new + beta_new + gamma_new + delta_new
            
            return {
                "alpha": alpha_new / total,
                "beta": beta_new / total,
                "gamma": gamma_new / total,
                "delta": delta_new / total
            }
        
        return {
            "alpha": self.alpha,
            "beta": self.beta,
            "gamma": self.gamma,
            "delta": self.delta
        }


# Test iÃ§in
if __name__ == "__main__":
    engine = RecommendationEngine()
    
    # Test user
    test_profile = {
        "score": 50,
        "level": "competent",
        "domain": "technical",
        "responses": {"ai_experience": True}
    }
    
    user_vec = engine.create_user_vector(test_profile)
    
    # Rank personas
    rankings = engine.rank_personas(user_vec, task_complexity=0.5, top_k=5)
    
    print("ğŸ¯ Top 5 Persona Tavsiyeleri:\n")
    for idx, ranking in enumerate(rankings, 1):
        persona_id = ranking["persona_id"]
        score = ranking["score"]
        components = ranking["components"]
        
        print(f"{idx}. {persona_id}: {score:.3f}")
        print(f"   Benzerlik: {components['similarity']:.3f}")
        print(f"   Yetkinlik Uyumu: {components['competency_match']:.3f}")
        print(f"   Performans Tahmini: {components['performance_prediction']:.3f}")
        print(f"   Ã–ÄŸrenme YÃ¶rÃ¼ngesi: {components['learning_trajectory']:.3f}")
        print()

